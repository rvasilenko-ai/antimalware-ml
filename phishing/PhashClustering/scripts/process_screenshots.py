"""
Process screenshots of phishing web sites downloaded from
https://nex.sx/blog/2019/12/15/the-year-of-the-phish.html

Torrent magnet link:
magnet:?xt=urn:btih:28f02613928c2666f7a8f70be4079c1084012cbb&dn=phishing.zip&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80

to compute image hashes and extract metadata to CSV file ready for further processing

Copyright:
    Roman Vasilenko, April 2020
"""

import argparse
import sys
import os
import io
from PIL import Image
import logging
import imagehash
import sqlite3
import pandas as pd
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class Error(Exception):
    """ Base error for the script. """


class InputDirectoryUnavailable(Error):
    """ Input directory unavailable. """


HASH_TYPE__PHASH = "phash"
SUPPORTED_HASH_TYPES = {
    HASH_TYPE__PHASH: imagehash.phash,
}
SUPPORTED_HASH_SIZE = [8, 16, 32, 64]


def lookup_target_data(conn, sha1):
    c = conn.cursor()
    t = (sha1,)

    c.execute("SELECT url, final_url, brand, date FROM urls WHERE sha1=?", t)
    url, final_url, target_label, visit_date = c.fetchone()
    if not target_label:
        target_label = "unknown"
    return url, final_url, target_label, visit_date


def process_image(image_path, max_dim, hash_type, hash_sizes):
    im_hashes = {}

    im = Image.open(image_path)
    if max_dim:
        im.thumbnail((max_dim, max_dim), Image.ANTIALIAS)

    with io.BytesIO() as output:
        im.save(output, format = 'PNG')
        sha1_hasher = hashlib.sha1()
        sha1_hasher.update(output.getbuffer())
        im_hashes['sha1'] = sha1_hasher.hexdigest()

    hash_func = SUPPORTED_HASH_TYPES[hash_type]
    for hash_size in hash_sizes:
        hash_id = "{}{}".format(hash_type, hash_size)
        im_hashes[hash_id] = str(hash_func(im, hash_size))

    return im, im_hashes


def get_hash_sizes(min_size, max_size):
    return [
        hash_size
        for hash_size in SUPPORTED_HASH_SIZE
        if min_size <= hash_size <= max_size
    ]


def do_main(argv):
    """
    Script main function
    :param argv: command line arguments
    :return:
    """
    parser = argparse.ArgumentParser(
        description="Process images using hashing algorithm"
    )
    parser.add_argument(
        "-i",
        "--input-dir",
        dest="input_dir",
        default=None,
        required=True,
        help="Input directory containing images to process.",
    )
    parser.add_argument(
        "--export-to-csv",
        dest="export_to_csv",
        default=None,
        required=True,
        help="Path to output CSV file to export data",
    )
    parser.add_argument(
        "--max-dim",
        dest="max_dim",
        default=300,
        type=int,
        help="Image maximum dimension to resize while keeping ratio",
    )
    parser.add_argument(
        "--max-hash-size",
        dest="max_hash_size",
        default=SUPPORTED_HASH_SIZE[-1],
        type=int,
        choices=SUPPORTED_HASH_SIZE,
        help="Max size of the hash for the hashing algorithm",
    )
    parser.add_argument(
        "--min-hash-size",
        dest="min_hash_size",
        default=SUPPORTED_HASH_SIZE[0],
        type=int,
        choices=SUPPORTED_HASH_SIZE,
        help="Min size of the hash for the hashing algorithm",
    )
    parser.add_argument(
        "--hash-type",
        dest="hash_type",
        default=HASH_TYPE__PHASH,
        choices=SUPPORTED_HASH_TYPES,
        help="Type of the hashing algorithm. Phash is used by default",
    )
    parser.add_argument(
        "--url-sqlite-db",
        dest="url_sqlite_db",
        default=None,
        help="Path to sqlite3 DB containing url metadata",
    )
    parser.add_argument(
        "--store-processed-images-dir",
        dest="store_processed_image_dir",
        default=None,
        help="Output directory to store clustered processed images. Clustering by shortest hash.",
    )
    parser.add_argument(
        "--limit",
        dest="limit",
        default=None,
        type=int,
        help="Set limit on number of processed screenshots",
    )

    args = parser.parse_args()

    if not os.path.isdir(args.store_processed_image_dir):
        raise InputDirectoryUnavailable(
            "Failed to access output directory: {}".format(args.store_processed_image_dir)
        )

    if not os.path.isdir(args.input_dir):
        raise InputDirectoryUnavailable(
            "Failed to access input directory: {}".format(args.input_dir)
        )

    image_dirs = os.listdir(args.input_dir)
    hash_sizes = get_hash_sizes(args.min_hash_size, args.max_hash_size)

    df_dict = {
        "site_sha1": [],
    }

    db_conn = None
    try:
        if args.url_sqlite_db:
            db_conn = sqlite3.connect(args.url_sqlite_db)

        num_images = len(image_dirs)
        idx_image = 0
        for site_sha1 in image_dirs:
            logger.info(
                "processing image: %s [%d/%d]", site_sha1, idx_image, num_images
            )
            if args.limit and args.limit <= idx_image:
                logger.info(
                    "Reach the limit on number of processes screenshots. Stop processing!"
                )
                break

            try:
                im, im_hashes = process_image(
                    os.path.join(args.input_dir, site_sha1, "screen.png"),
                    args.max_dim,
                    args.hash_type,
                    hash_sizes,
                )

                df_dict["site_sha1"].append(site_sha1)
                for hash_id, hash_value in im_hashes.items():
                    if hash_id not in df_dict:
                        df_dict[hash_id] = []
                    df_dict[hash_id].append(hash_value)

                if db_conn:
                    try:
                        url, final_url, target_label, visit_date = lookup_target_data(
                            db_conn, site_sha1
                        )
                        if "url" not in df_dict:
                            df_dict["url"] = []
                            df_dict["final_url"] = []
                            df_dict["target_label"] = []
                            df_dict["date"] = []
                        df_dict["url"].append(url)
                        df_dict["final_url"].append(final_url)
                        df_dict["target_label"].append(target_label)
                        df_dict["date"].append(visit_date)
                    except Error as err:
                        logger.error(
                            "Unable to get URL date for %s. Error: %s", site_sha1, err
                        )

                if args.store_processed_image_dir:
                    try:
                        image_path = os.path.join(
                            args.store_processed_image_dir, "{}.png".format(im_hashes['sha1'])
                        )
                        if not os.path.isfile(image_path):
                            im.save(os.path.join(image_path), format = 'PNG')
                    except Error as err:
                        logger.error(
                            "Unable to store processed screenshot for %s. Error: %s",
                            site_sha1,
                            err,
                        )

                idx_image = idx_image + 1
            except FileNotFoundError as err:
                logger.error("Unable to open screenshot: %s", err)

        if args.export_to_csv:
            df = pd.DataFrame(data=df_dict)
            if args.export_to_csv:
                df.to_csv(args.export_to_csv)
    finally:
        if db_conn:
            db_conn.close()


if __name__ == "__main__":
    try:
        do_main(sys.argv)
    except KeyboardInterrupt:
        logger.info("Processing interrupted by used!")
